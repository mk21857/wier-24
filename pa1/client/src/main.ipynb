{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02829958",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.options import Options as FirefoxOptions\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "import urllib3\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "import warnings\n",
    "import threading\n",
    "import hashlib\n",
    "import selenium\n",
    "import urllib\n",
    "from urllib.parse import urlparse, urlunparse, urldefrag\n",
    "from bs4 import BeautifulSoup\n",
    "import threading\n",
    "from queue import Queue\n",
    "import urllib.robotparser\n",
    "import datetime\n",
    "import socket\n",
    "import errno\n",
    "import api_calls as ac\n",
    "print(selenium.__version__)\n",
    "\n",
    "\n",
    "# Options and parameters\n",
    "WEB_DRIVER_LOCATION = \"/app/geckodriver\"\n",
    "TIMEOUT = 5\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "session = requests.Session()\n",
    "session.mount('http://', HTTPAdapter(max_retries=1))\n",
    "session.mount('https://', HTTPAdapter(max_retries=2))\n",
    "\n",
    "INITIAL_SEED = [\n",
    "    #'https://www.gov.si',\n",
    "    #'https://spot.gov.si',\n",
    "    #'https://e-uprava.gov.si',\n",
    "    #'https://www.e-prostor.gov.si'\n",
    "]\n",
    "\n",
    "NUMBER_OF_WORKERS = 1\n",
    "\n",
    "GROUP_NAME='wier2024'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "33120cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Site:\n",
    "    def __init__(self, domain, robots_content, sitemap_content):\n",
    "        self.domain = domain\n",
    "        self.robots_content = robots_content\n",
    "        self.sitemap_content = sitemap_content\n",
    "        \n",
    "class Page:\n",
    "    def __init__(self, url, domain, page_type_code=None, content_hash=None, http_status_code=None, accessed_time=None, data_type_code=None, html_content=None, robots_content=None, from_page=None):\n",
    "        self.page_type_code = page_type_code\n",
    "        self.url = url\n",
    "        self.html_content = html_content\n",
    "        self.content_hash = content_hash\n",
    "        self.http_status_code = http_status_code\n",
    "        self.accessed_time = accessed_time\n",
    "        self.domain = domain\n",
    "        self.data_type_code = data_type_code\n",
    "        self.robots_content = robots_content\n",
    "        self.sitemap_content = \"\"\n",
    "        self.from_page = from_page\n",
    "        \n",
    "class Image:\n",
    "    def __init__(self, filename, content_type, data, accessed_time):\n",
    "        self.filename = filename\n",
    "        self.content_type = content_type\n",
    "        self.data = data\n",
    "        self.accessed_time = accessed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c32d71ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrawlerManager:\n",
    "    def __init__(self, initial_seed, num_workers):\n",
    "        self.num_workers = num_workers\n",
    "        self.initial_seed = initial_seed\n",
    "        self.frontier = Queue()\n",
    "        \n",
    "    def get_frontier(self):\n",
    "        # Call database to get all the frontier entries from Page table\n",
    "        # For each push into frontier.Queue()\n",
    "        self.frontier = Queue()\n",
    "        frontier_elements = ac.get_frontier()\n",
    "        for element in frontier_elements:\n",
    "            self.frontier.put(Page(element[1], self.get_domain(element[1])))\n",
    "        \n",
    "        print('Frontier size: ', self.frontier.qsize())\n",
    "        \n",
    "    def get_domain(self, url):\n",
    "        parsed_url = urlparse(url)\n",
    "        domain = parsed_url.netloc\n",
    "        if \"www.\" in domain:\n",
    "            domain = domain.replace(\"www.\", \".\")\n",
    "            \n",
    "        return domain\n",
    "        \n",
    "    def set_frontier(self):\n",
    "        print('Setting frontier')\n",
    "\n",
    "        # Search the database\n",
    "        # Pages with PAGE_TYPE = FRONTIER\n",
    "        \n",
    "        # self.frontier.put(page)\n",
    "                \n",
    "        #print('Frontier size: ', self.frontier.qsize())\n",
    "        \n",
    "    def initialize_seed(self):\n",
    "        #print('Initializing seed')\n",
    "        # Add URLs to frontier passed from the argument\n",
    "        # In case there are no URLs passed from the argument, search for the seed URLs in the database\n",
    "        if len(self.initial_seed) != 0:\n",
    "            for url in self.initial_seed:\n",
    "                #  Insert page into the frontier to DB\n",
    "                #page = Page(url, self.get_domain(url))\n",
    "                \n",
    "                data = {\n",
    "                        \"domain\": self.get_domain(url),\n",
    "                        \"url\": url,\n",
    "                        \"from_page\": None\n",
    "                }\n",
    "                page_id = ac.insert_page_into_frontier(data)\n",
    "                # Insert each page into frontier\n",
    "                \n",
    "        self.get_frontier()\n",
    "        \n",
    "    def run(self):\n",
    "        while not self.frontier.empty():\n",
    "            count = 0\n",
    "            threads = []\n",
    "            while not self.frontier.empty() and count < self.num_workers:\n",
    "                #if count > self.num_workers:\n",
    "                #    break\n",
    "\n",
    "                # Dequeue element\n",
    "                page = self.frontier.get()\n",
    "                crawler = Crawler()\n",
    "\n",
    "                # Process the element\n",
    "                thread = threading.Thread(name=f'{count}', target=crawler.crawl_webpage, args=(page,))\n",
    "                thread.start()\n",
    "                threads.append(thread)\n",
    "\n",
    "                count += 1\n",
    "            \n",
    "            for thread in threads:\n",
    "                thread.join()\n",
    "    \n",
    "            # Get frontier from database until frontier empty\n",
    "            self.get_frontier()\n",
    "    \n",
    "        print('Crawler finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dffae1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_request_times = {}\n",
    "\n",
    "class Crawler:\n",
    "    def __init__(self):#, initial_seed, num_workers):\n",
    "        #self.initial_seed = initial_seed\n",
    "        #self.frontier = Queue()\n",
    "        #self.num_workers = num_workers\n",
    "        self.web_driver = self.initialize_web_driver()\n",
    "        self.thread = None\n",
    "        self.disallowed_urls = []\n",
    "    \n",
    "    def initialize_web_driver(self):\n",
    "        options = FirefoxOptions()\n",
    "        options.add_argument(\"--headless\")\n",
    "        options.add_argument(\"user-agent=fri-wier-oskapha\")\n",
    "        service = Service(executable_path=WEB_DRIVER_LOCATION)\n",
    "        driver = webdriver.Firefox(service=service, options=options)\n",
    "        driver.set_page_load_timeout(10)\n",
    "        return driver\n",
    "        \n",
    "    def canonicalize_url(self, url):\n",
    "        \"\"\"\n",
    "        Canonicalizes a URL by removing redundant parts and ensuring consistency.\n",
    "        \"\"\"\n",
    "        parsed_url = urlparse(url)\n",
    "        parsed_url = parsed_url._replace(fragment='')\n",
    "        parsed_url = parsed_url._replace(scheme=parsed_url.scheme.lower(), netloc=parsed_url.netloc.lower())\n",
    "        parsed_url = parsed_url._replace(query='')\n",
    "        \n",
    "        if parsed_url.path.endswith('/'):\n",
    "            parsed_url = parsed_url._replace(path=parsed_url.path[:-1])\n",
    "        if parsed_url.path.endswith('/index.html'):\n",
    "            parsed_url = parsed_url._replace(path=parsed_url.path[:-11])\n",
    "        if parsed_url.path.endswith('/index.php'):\n",
    "            parsed_url = parsed_url._replace(path=parsed_url.path[:-10])\n",
    "        \n",
    "        if parsed_url.scheme == 'http' or parsed_url.scheme == 'https':\n",
    "            pass\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "        parsed_url = parsed_url._replace(path=urlparse(parsed_url.geturl()).path)\n",
    "        \n",
    "        canonicalized_url = urlunparse(parsed_url)\n",
    "        \n",
    "        return canonicalized_url\n",
    "    \n",
    "    def parse_robots(self, url):\n",
    "        domain = self.get_domain(url)\n",
    "        robot_parser = urllib.robotparser.RobotFileParser()\n",
    "        if domain.startswith('.'):\n",
    "            domain = domain[1:]\n",
    "        robot_parser.set_url(f'https://{domain}/robots.txt')\n",
    "        robot_parser.read()\n",
    "        crawl_delay = robot_parser.crawl_delay('')  # '' for useragent\n",
    "        site_maps = robot_parser.site_maps()\n",
    "        \n",
    "        response = requests.get(f'https://{domain}/robots.txt', timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            robots_content = response.text\n",
    "        else:\n",
    "            robots_content = None\n",
    "        return crawl_delay, site_maps, robots_content\n",
    "            \n",
    "    \n",
    "    def robots_allow(self, url):\n",
    "        domain = self.get_domain(url)\n",
    "        robot_parser = urllib.robotparser.RobotFileParser()\n",
    "        if domain.startswith('.'):\n",
    "            domain = domain[1:]\n",
    "        robot_parser.set_url(f'https://{domain}/robots.txt')\n",
    "        robot_parser.read()\n",
    "        return robot_parser.can_fetch(GROUP_NAME, url)\n",
    "    \n",
    "    def check_duplicates(self, page):\n",
    "        # Check if the page is already in the database by checking the content hash\n",
    "        hashed_content = self.hash_content(page.html_content)\n",
    "        \n",
    "        content_hashes = []\n",
    "        content_hashes = ac.get_hashed_content()\n",
    "        # Get function to get the content hash of all pages in the database\n",
    "        if hashed_content in content_hashes:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def request_timeout(self, ip):\n",
    "        last_request_time = last_request_times.get(ip)\n",
    "        if last_request_time is None:\n",
    "            return\n",
    "        current_time = time.time()\n",
    "        timeout = current_time - last_request_time\n",
    "        if timeout < 5:\n",
    "            time.sleep(5 - timeout)\n",
    "    \n",
    "    def update_last_request_time(self, ip):\n",
    "        last_request_times[ip] = time.time()\n",
    "    \n",
    "    def get_domain(self, url):\n",
    "        parsed_url = urlparse(url)\n",
    "        domain = parsed_url.netloc\n",
    "        if \"www.\" in domain:\n",
    "            domain = domain.replace(\"www.\", \".\")\n",
    "            \n",
    "        return domain\n",
    "    \n",
    "    def hash_content(self, content):\n",
    "        hashed_content = hashlib.md5(content.encode('utf-8')).hexdigest()\n",
    "        #print(hashed_content)\n",
    "        return hashed_content\n",
    "    \n",
    "    def fetch_content(self, url):\n",
    "        response = requests.get(url, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            return response.content\n",
    "        else:\n",
    "            print('Fetching content failed')\n",
    "            return None\n",
    "        \n",
    "    def get_images(self):\n",
    "        image_objects = []\n",
    "        try:\n",
    "            images = self.web_driver.find_elements(by=By.TAG_NAME, value=\"img\")\n",
    "\n",
    "            for image in images:\n",
    "                filename = image.get_attribute('src')\n",
    "                if filename is not None:\n",
    "                    content_type = filename.split(\".\")[-1]\n",
    "                    if len(content_type) > 3:\n",
    "                        content_type = 'UNKNOWN'\n",
    "                    if 'base64' in content_type and len(content_type) > 3:\n",
    "                        metadata, base64_data = content_type.split(\",\")\n",
    "                        content_type = metadata.split(\";\")[0].split(\":\")[1].split(\"/\")[1]\n",
    "                        content = base64_data\n",
    "                    elif len(content_type) < 4:\n",
    "                        # There is no need to populate content in database\n",
    "                        #content = self.fetch_content(filename)\n",
    "                        content = 'something'\n",
    "                        \n",
    "                    accessed_time = datetime.datetime.now()\n",
    "                    image_object = Image(filename, content_type, content, accessed_time)\n",
    "                    image_objects.append(image_object)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        return image_objects\n",
    "        \n",
    "    def validate_links(self, links):\n",
    "        already_added_links = set()\n",
    "        validated_links = []\n",
    "        for link in links:\n",
    "            if link is not None and len(link) > 0 and 'javascript:' not in link.lower() and 'mailto:' not in link.lower():\n",
    "                domain = self.get_domain(link)\n",
    "                #print('Domain: ', domain)\n",
    "                if \"gov.si\" in domain: # Add robots check before appending\n",
    "                    \n",
    "                    canonicalized_url = self.canonicalize_url(link)\n",
    "                    #print('Canonicalized: ', canonicalized_url)\n",
    "                    \n",
    "                    # Check for duplicates in the array\n",
    "                    if (canonicalized_url not in already_added_links and canonicalized_url not in INITIAL_SEED):\n",
    "                        #print('Valid')\n",
    "                        validated_links.append(canonicalized_url)\n",
    "                        already_added_links.add(canonicalized_url)\n",
    "            \n",
    "        return validated_links\n",
    "        \n",
    "    # Get all urls from links on the page\n",
    "    def get_links(self, page):\n",
    "        #print('Getting links')\n",
    "        links = []\n",
    "        \n",
    "        # Check robots again\n",
    "        _, sitemap_links, _ = self.parse_robots(page.url)\n",
    "        # Links from sitemaps\n",
    "        if sitemap_links is not None:\n",
    "            for sitemap_link in sitemap_links:\n",
    "                #print(\"Sitemap: \", sitemap_link)\n",
    "                links.append(sitemap_link)\n",
    "            \n",
    "        # Get links from href\n",
    "        try:\n",
    "            hrefs = self.web_driver.find_elements(by=By.XPATH, value=\"//a[@href]\")\n",
    "            for href in hrefs:\n",
    "                try:\n",
    "                    href_value = href.get_attribute(\"href\")\n",
    "                    links.append(href_value)\n",
    "                except StaleElementReferenceException as e:\n",
    "                    pass\n",
    "        except NoSuchElementException:\n",
    "            print('No elements')    \n",
    "        \n",
    "        # Get links from buttons\n",
    "        try:\n",
    "            on_clicks = self.web_driver.find_elements(by=By.XPATH, value=\"//*[@onclick]\")\n",
    "            for on_click in on_clicks:\n",
    "                try:\n",
    "                    click_value = on_click.get_attribute('onclick')\n",
    "                    \n",
    "                    # Check for links\n",
    "                    if 'document.location' in click_value or 'location.href' in click_value:\n",
    "                        link = click_value.split('=')[1]\n",
    "                        links.append(link)\n",
    "                except StaleElementReferenceException as e:\n",
    "                    pass\n",
    "        except NoSuchElementException:\n",
    "            print('No elements')\n",
    "        \n",
    "        validated_links = self.validate_links(links)\n",
    "            \n",
    "        return validated_links\n",
    "            \n",
    "    def crawl_webpage(self, page):\n",
    "        self.thread = threading.current_thread()\n",
    "        print(f'Thread: {self.thread.name} now crawling: {page.url}')\n",
    "        \n",
    "        try:\n",
    "            # Implement robots.txt\n",
    "            if not self.robots_allow(page.url):\n",
    "                return\n",
    "            \n",
    "            crawl_delay, sitemap_links, robots_content = self.parse_robots(page.url)\n",
    "            page.robots_content = robots_content\n",
    "            \n",
    "            if sitemap_links is not None:\n",
    "                for sitemap_link in sitemap_links:\n",
    "                    page.sitemap_content += sitemap_link + \"\\n\"\n",
    "            else:\n",
    "                page.sitemap_content = \"\"\n",
    "            \n",
    "            try:\n",
    "                ip = socket.gethostbyname(page.domain)\n",
    "                #print('IP: ', ip)\n",
    "            except Exception as e:\n",
    "                ip = None\n",
    "            \n",
    "            if crawl_delay is None:\n",
    "                self.request_timeout(ip)\n",
    "            else:\n",
    "                time.sleep(crawl_delay)\n",
    "            \n",
    "            # Read the page\n",
    "            try:\n",
    "                response = requests.head(page.url, allow_redirects=True, timeout=10)\n",
    "                page.http_status_code = response.status_code\n",
    "                page.accessed_time = datetime.datetime.now()\n",
    "                page.content_type = response.headers['Content-Type']\n",
    "                \n",
    "            except Exception as e:\n",
    "                # Incase there are problems with header try the whole get\n",
    "                response = requests.get(page.url, allow_redirects=True, timeout=10)\n",
    "                page.http_status_code = response.status_code\n",
    "                page.accessed_time = datetime.datetime.now()\n",
    "                page.content_type = response.headers['Content-Type']\n",
    "            \n",
    "            # Is it HTML\n",
    "            if \"text/html\" in response.headers['Content-Type']:\n",
    "                page.page_type_code = \"HTML\"\n",
    "                self.web_driver.get(page.url)\n",
    "                page.html_content = self.web_driver.page_source\n",
    "                page.content_hash = self.hash_content(page.html_content)\n",
    "                page.data_type = None\n",
    "                \n",
    "                # Check duplicates\n",
    "                #if self.check_duplicates(page):\n",
    "                #    print('Duplicates found')\n",
    "                #    page.page_type_code = \"DUPLICATE\"\n",
    "                \n",
    "                # Check links\n",
    "                links = self.get_links(page)\n",
    "                \n",
    "                # Check for images\n",
    "                try:\n",
    "                    images = self.get_images()\n",
    "                    # Insert image data into database\n",
    "                    for image in images:\n",
    "                        # Insert each image\n",
    "                        #print('Item ', image.filename)\n",
    "                        data = {\n",
    "                            \"url\": page.url,\n",
    "                            \"filename\": image.filename,\n",
    "                            \"content_type\": image.content_type,\n",
    "                            \"accessed_time\": image.accessed_time.isoformat()\n",
    "                        }\n",
    "                        ac.insert_image(data)\n",
    "                except Exception as e:\n",
    "                    print('Error on images: ', e)\n",
    "                \n",
    "                # Insert links into frontier\n",
    "                for link in links:\n",
    "                    data = {\n",
    "                        \"domain\": self.get_domain(link),\n",
    "                        \"url\": link,\n",
    "                        \"from_page\": page.url\n",
    "                    }\n",
    "                    page_id = ac.insert_page_into_frontier(data)\n",
    "                    #print(page_id)\n",
    "                \n",
    "            else:\n",
    "                # Else it is a binary file\n",
    "                #print(\"It's a binary file!\")\n",
    "                \n",
    "                page.html_content = None\n",
    "                page.page_type_code = \"BINARY\"\n",
    "                if \"application/pdf\" == page.content_type:\n",
    "                    page.data_type = \"PDF\"\n",
    "                    \n",
    "                elif \"application/msword\" == page.content_type:\n",
    "                    page.data_type = \"DOC\"\n",
    "                \n",
    "                elif \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\" == page.content_type:\n",
    "                    page.data_type = \"DOCX\"\n",
    "                    \n",
    "                elif \"application/vnd.ms-powerpoint\" == page.content_type:\n",
    "                    page.data_type = \"PPT\"\n",
    "                    \n",
    "                elif \"application/vnd.openxmlformats-officedocument.presentationml.presentation\" == page.content_type:\n",
    "                    page.data_type = \"PPTX\"\n",
    "                \n",
    "                else:\n",
    "                    page.data_type = \"UNDEFINED\"\n",
    "                \n",
    "            self.update_last_request_time(ip)\n",
    "            \n",
    "            # Insert into database table Page Data since it is a binary file\n",
    "            # Also update the page entry in table Page\n",
    "            data = {\n",
    "                \"url\": page.url,\n",
    "                \"page_type_code\": page.page_type_code,\n",
    "                \"html_content\": page.html_content,\n",
    "                \"http_status_code\": page.http_status_code,\n",
    "                \"accessed_time\": page.accessed_time.isoformat(),\n",
    "                \"robots_content\": page.robots_content,\n",
    "                \"sitemap_content\": page.sitemap_content,\n",
    "                \"data_type_code\": page.data_type,\n",
    "                \"hashed_content\": page.content_hash\n",
    "                \n",
    "            }\n",
    "            print(ac.update_page_data(data))\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Update database page entry with PAGE_TYPE = ERROR\n",
    "            page.page_type_code = \"ERROR\"\n",
    "            page.data_type = \"UNDEFINED\"\n",
    "            data = {\n",
    "                \"url\": page.url,\n",
    "                \"page_type_code\": page.page_type_code,\n",
    "                \"html_content\": page.html_content,\n",
    "                \"http_status_code\": page.http_status_code,\n",
    "                \"accessed_time\": datetime.datetime.now().isoformat(),\n",
    "                \"robots_content\": page.robots_content,\n",
    "                \"sitemap_content\": page.sitemap_content,\n",
    "                \"data_type_code\": page.data_type,\n",
    "                \"hashed_content\": page.content_hash\n",
    "                \n",
    "            }\n",
    "            print('Error crawling', page.url, e)\n",
    "            print('Update on error: ', ac.update_page_data(data))\n",
    "            \n",
    "            return\n",
    "        \n",
    "    def __del__(self):\n",
    "        if self.web_driver is not None:\n",
    "            self.web_driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7d6139b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m crawler_manager \u001b[38;5;241m=\u001b[39m CrawlerManager(initial_seed\u001b[38;5;241m=\u001b[39mINITIAL_SEED, num_workers\u001b[38;5;241m=\u001b[39mNUMBER_OF_WORKERS)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Initialize seed and create first Page entries\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mcrawler_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Get frontier and run the loop\u001b[39;00m\n\u001b[1;32m      7\u001b[0m crawler_manager\u001b[38;5;241m.\u001b[39mrun()\n",
      "Cell \u001b[0;32mIn[80], line 52\u001b[0m, in \u001b[0;36mCrawlerManager.initialize_seed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m         page_id \u001b[38;5;241m=\u001b[39m ac\u001b[38;5;241m.\u001b[39minsert_page_into_frontier(data)\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;66;03m# Insert each page into frontier\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_frontier\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[80], line 11\u001b[0m, in \u001b[0;36mCrawlerManager.get_frontier\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_frontier\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Call database to get all the frontier entries from Page table\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# For each push into frontier.Queue()\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfrontier \u001b[38;5;241m=\u001b[39m Queue()\n\u001b[0;32m---> 11\u001b[0m     frontier_elements \u001b[38;5;241m=\u001b[39m \u001b[43mac\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_frontier\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m frontier_elements:\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfrontier\u001b[38;5;241m.\u001b[39mput(Page(element[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_domain(element[\u001b[38;5;241m1\u001b[39m])))\n",
      "File \u001b[0;32m/app/api_calls.py:14\u001b[0m, in \u001b[0;36mget_frontier\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_frontier\u001b[39m():\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mENDPOINT\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/get_frontier_pages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mAUTH\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'data'"
     ]
    }
   ],
   "source": [
    "NUMBER_OF_WORKERS = 10\n",
    "crawler_manager = CrawlerManager(initial_seed=INITIAL_SEED, num_workers=NUMBER_OF_WORKERS)\n",
    "\n",
    "# Initialize seed and create first Page entries\n",
    "crawler_manager.initialize_seed()\n",
    "# Get frontier and run the loop\n",
    "crawler_manager.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbaf71d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
