{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "02829958",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.18.1\n",
      "Connected to database!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/wier/lib/python3.9/site-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Connect to Flask API\n",
    "\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.options import Options as FirefoxOptions\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "\n",
    "# Check the selenium version insalled\n",
    "import selenium\n",
    "print(selenium.__version__)\n",
    "\n",
    "WEB_DRIVER_LOCATION = \"/app/geckodriver\"\n",
    "TIMEOUT = 5\n",
    "\n",
    "#firefox_options = FirefoxOptions()\n",
    "# If you comment the following line, a browser will show ...\n",
    "#firefox_options.add_argument(\"--headless\")\n",
    "\n",
    "# Adding a specific user agent\n",
    "#firefox_options.add_argument(\"user-agent=fri-ieps-TEST\")\n",
    "\n",
    "#service = Service(executable_path=WEB_DRIVER_LOCATION)\n",
    "#driver = webdriver.Firefox(service=service, options=firefox_options)\n",
    "#driver.get(\"https://vreme.arso.gov.si\")\n",
    "#time.sleep(TIMEOUT)\n",
    "\n",
    "#html = driver.page_source\n",
    "\n",
    "# print(f\"Retrieved Web content (truncated to first 900 chars): \\n\\n'\\n{html[:900]}\\n'\\n\")\n",
    "\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "\n",
    "session = requests.Session()\n",
    "session.mount('http://', HTTPAdapter(max_retries=1))\n",
    "session.mount('https://', HTTPAdapter(max_retries=2))\n",
    "\n",
    "AUTH = (\"admin\", \"admin\") # Development authentication\n",
    "ENDPOINT = \"https://api:5000\" # Accessible inside Docker network\n",
    "\n",
    "response = requests.get(ENDPOINT + \"/\", verify = False, auth = AUTH)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(response.json()['message'])\n",
    "else:\n",
    "    print(response.json()['message'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "33120cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Site:\n",
    "    def __init__(self, domain, robots_content, sitemap_content, crawl_delay):\n",
    "        self.domain = domain\n",
    "        self.robots_content = robots_content\n",
    "        self.sitemap_content = sitemap_content\n",
    "        self.crawl_dielay = crawl_delay\n",
    "        \n",
    "class Page:\n",
    "    def __init__(self, url, domain, page_type_code=None, content_hash=None, http_status_code=None, accessed_time=None, data_type_code=None, html_content=None):\n",
    "        self.page_type_code = page_type_code\n",
    "        self.url = url\n",
    "        self.html_content = html_content\n",
    "        self.content_hash = content_hash\n",
    "        self.http_status_code = http_status_code\n",
    "        self.accessed_time = accessed_time\n",
    "        self.domain = domain\n",
    "        self.data_type_code = data_type_code\n",
    "        \n",
    "class Image:\n",
    "    def __init__(self, filename, content_type, data, accessed_time):\n",
    "        self.filename = filename\n",
    "        self.content_type = content_type\n",
    "        self.data = data\n",
    "        self.accessed_time = accessed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dffae1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from urllib.parse import urlparse, urlunparse\n",
    "from bs4 import BeautifulSoup\n",
    "import threading\n",
    "from queue import Queue\n",
    "import urllib.robotparser\n",
    "import datetime\n",
    "\n",
    "class Crawler:\n",
    "    def __init__(self, initial_seed, num_workers):\n",
    "        self.initial_seed = initial_seed\n",
    "        self.frontier = Queue()\n",
    "        # self.visited = set()\n",
    "        self.num_workers = num_workers\n",
    "        self.web_driver = self.initialize_web_driver()\n",
    "        # self.lock = threading.Lock()\n",
    "    \n",
    "    def initialize_web_driver(self):\n",
    "        options = FirefoxOptions()\n",
    "        options.add_argument(\"--headless\")\n",
    "        options.add_argument(\"user-agent=fri-wier-oskapha\")\n",
    "        service = Service(executable_path=WEB_DRIVER_LOCATION)\n",
    "        driver = webdriver.Firefox(service=service, options=options)\n",
    "        driver.set_page_load_timeout(10)\n",
    "        return driver\n",
    "        \n",
    "    def canonicalize_url(self, url):\n",
    "        \"\"\"\n",
    "        Canonicalizes a URL by removing redundant parts and ensuring consistency.\n",
    "        \"\"\"\n",
    "        parsed_url = urlparse(url)\n",
    "        parsed_url = parsed_url._replace(fragment='')\n",
    "        parsed_url = parsed_url._replace(scheme=parsed_url.scheme.lower(), netloc=parsed_url.netloc.lower())\n",
    "        parsed_url = parsed_url._replace(path=urlparse(parsed_url.geturl()).path)\n",
    "        \n",
    "        if parsed_url.path and not parsed_url.path.endswith('/'):\n",
    "            parsed_url = parsed_url._replace(path=parsed_url.path + '/')\n",
    "        \n",
    "        canonicalized_url = urlunparse(parsed_url)\n",
    "        \n",
    "        return canonicalized_url\n",
    "    \n",
    "    def robots_allow(self, url):\n",
    "        domain = self.get_domain(url)\n",
    "        robot_parser = urllib.robotparser.RobotFileParser()\n",
    "        robot_parser.set_url(f'https://{domain}/robots.txt')\n",
    "        robot_parser.read()\n",
    "        return robot_parser.can_fetch(GROUP_NAME, url)\n",
    "    \n",
    "    def check_duplicates(self, page):\n",
    "        # Check if the page is already in the database by checking the content hash\n",
    "        content_hashes = []\n",
    "        # Get function to get the content hash of all pages in the database\n",
    "        \n",
    "        for content_hash in content_hashes:\n",
    "            if content_hash == page.content_hash:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "    \n",
    "    def get_domain(self, url):\n",
    "        parsed_url = urlparse(url)\n",
    "        domain = parsed_url.netloc\n",
    "        if \"www.\" in domain:\n",
    "            domain = domain.replace(\"www.\", \"\")\n",
    "            \n",
    "        return domain\n",
    "        \n",
    "    def set_frontier(self):\n",
    "        print('Setting frontier')\n",
    "\n",
    "        # Search the database\n",
    "        # Pages with PAGE_TYPE = FRONTIER\n",
    "        \n",
    "        # self.frontier.put(page)\n",
    "                \n",
    "        print('Frontier size: %s', self.frontier.qsize())\n",
    "        \n",
    "    def initialize_seed(self):\n",
    "        print('Initializing seed')\n",
    "        # Add URLs to frontier passed from the argument\n",
    "        # In case there are no URLs passed from the argument, search for the seed URLs in the database\n",
    "        if len(self.initial_seed) != 0:\n",
    "            for url in self.initial_seed:\n",
    "                #  Insert page into the frontier to DB\n",
    "                page = Page(url, self.get_domain(url))\n",
    "                \n",
    "                # Insert each page into frontier\n",
    "                \n",
    "                self.frontier.put(page)\n",
    "                \n",
    "        print('Frontier size: %s', self.frontier.qsize())\n",
    "        \n",
    "    def get_links(page):\n",
    "        print('Getting links')\n",
    "        links = []\n",
    "            \n",
    "        return links\n",
    "                \n",
    "    def crawl_webpage(self, page):\n",
    "        print('Started crawling', page.url)\n",
    "        \n",
    "        try:\n",
    "            # Implement robots.txt\n",
    "            \n",
    "            # Read the page\n",
    "            \n",
    "            response = requests.head(page.url, allow_redirects=True, timeout=10)\n",
    "            page.http_status_code = response.status_code\n",
    "            page.accessed_time = datetime.datetime.now()\n",
    "            page.content_type = response.headers['Content-Type']\n",
    "            \n",
    "            # Is it HTML\n",
    "            if \"text/html\" in response.headers['Content-Type']:\n",
    "                page.page_type_code = \"HTML\"\n",
    "                self.web_driver.get(\"http://\" + page.domain)\n",
    "                page.html_content = self.web_driver.page_source\n",
    "                \n",
    "                \n",
    "                # Check duplicates\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                # Else it is a binary file\n",
    "                print(\"It's a binary file!\")\n",
    "                \n",
    "                page.html_content = None\n",
    "                page.page_type_code = \"BINARY\"\n",
    "                if \"application/pdf\" == page.content_type:\n",
    "                    page.data_type = \"PDF\"\n",
    "                    \n",
    "                elif \"application/msword\" == page.content_type:\n",
    "                    page.data_type = \"DOC\"\n",
    "                \n",
    "                elif \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\" == page.content_type:\n",
    "                    page.data_type = \"DOCX\"\n",
    "                    \n",
    "                elif \"application/vnd.ms-powerpoint\" == page.content_type:\n",
    "                    page.data_type = \"PPT\"\n",
    "                    \n",
    "                elif \"application/vnd.openxmlformats-officedocument.presentationml.presentation\" == page.content_type:\n",
    "                    page.data_type = \"PPTX\"\n",
    "            \n",
    "            # If everything went fine update database page entry\n",
    "            print('Crawled %s', response, page.content_type)\n",
    "        except Exception as e:\n",
    "            # Update database page entry with PAGE_TYPE = TIMEOUT\n",
    "            print('Error crawling %s: %s', page, e)\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bb23e398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start of crawler\n",
    "\n",
    "import urllib3\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "import warnings\n",
    "\n",
    "INITIAL_SEED = [\n",
    "    'https://www.gov.si/',\n",
    "    'https://spot.gov.si',\n",
    "    'https://e-uprava.gov.si',\n",
    "    'https://www.e-prostor.gov.si'\n",
    "]\n",
    "\n",
    "NUMBER_OF_WORKERS = 1\n",
    "\n",
    "GROUP_NAME='wier2024'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7cf48630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing seed\n",
      "Frontier size: %s 4\n"
     ]
    }
   ],
   "source": [
    "crawler = Crawler(initial_seed=INITIAL_SEED, num_workers=NUMBER_OF_WORKERS)\n",
    "crawler.initialize_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4bdee23a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started crawling https://www.gov.si/\n",
      "Crawled %s <Response [200]> text/html; charset=utf-8\n",
      "Started crawling https://spot.gov.si\n",
      "Crawled %s <Response [200]> text/html; charset=utf-8\n",
      "Started crawling https://e-uprava.gov.si\n",
      "Error crawling %s: %s <__main__.Page object at 0x7f8d38dc6190> 'content-type'\n",
      "Started crawling https://www.e-prostor.gov.si\n",
      "Crawled %s <Response [200]> text/html; charset=utf-8\n"
     ]
    }
   ],
   "source": [
    "while crawler.frontier.qsize() > 0:\n",
    "    # Dequeue element\n",
    "    element = crawler.frontier.get()\n",
    "    \n",
    "    # Process the element\n",
    "    crawler.crawl_webpage(page=element)\n",
    "    \n",
    "    crawler.frontier.task_done()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
