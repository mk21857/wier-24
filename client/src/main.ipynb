{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "02829958",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.19.0\n",
      "Connected to the database successfully.\n",
      "[(1, 'neki.vom')]\n"
     ]
    }
   ],
   "source": [
    "# Connect to Flask API\n",
    "\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.options import Options as FirefoxOptions\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "\n",
    "# Check the selenium version insalled\n",
    "import selenium\n",
    "print(selenium.__version__)\n",
    "\n",
    "WEB_DRIVER_LOCATION = \"/app/geckodriver\"\n",
    "TIMEOUT = 5\n",
    "\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "\n",
    "session = requests.Session()\n",
    "session.mount('http://', HTTPAdapter(max_retries=1))\n",
    "session.mount('https://', HTTPAdapter(max_retries=2))\n",
    "\n",
    "import database\n",
    "\n",
    "conn = database.connect_to_database()\n",
    "\n",
    "print(database.get_frontier_pages(conn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "33120cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Site:\n",
    "    def __init__(self, domain, robots_content, sitemap_content, crawl_delay):\n",
    "        self.domain = domain\n",
    "        self.robots_content = robots_content\n",
    "        self.sitemap_content = sitemap_content\n",
    "        self.crawl_dielay = crawl_delay\n",
    "        \n",
    "class Page:\n",
    "    def __init__(self, url, domain, page_type_code=None, content_hash=None, http_status_code=None, accessed_time=None, data_type_code=None, html_content=None):\n",
    "        self.page_type_code = page_type_code\n",
    "        self.url = url\n",
    "        self.html_content = html_content\n",
    "        self.content_hash = content_hash\n",
    "        self.http_status_code = http_status_code\n",
    "        self.accessed_time = accessed_time\n",
    "        self.domain = domain\n",
    "        self.data_type_code = data_type_code\n",
    "        \n",
    "class Image:\n",
    "    def __init__(self, filename, content_type, data, accessed_time):\n",
    "        self.filename = filename\n",
    "        self.content_type = content_type\n",
    "        self.data = data\n",
    "        self.accessed_time = accessed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dffae1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from urllib.parse import urlparse, urlunparse, urldefrag\n",
    "from bs4 import BeautifulSoup\n",
    "import threading\n",
    "from queue import Queue\n",
    "import urllib.robotparser\n",
    "import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "class Crawler:\n",
    "    def __init__(self, initial_seed, num_workers):\n",
    "        self.initial_seed = initial_seed\n",
    "        self.frontier = Queue()\n",
    "        # self.visited = set()\n",
    "        self.num_workers = num_workers\n",
    "        self.web_driver = self.initialize_web_driver()\n",
    "        # self.lock = threading.Lock()\n",
    "    \n",
    "    def initialize_web_driver(self):\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument(\"--headless\")\n",
    "        options.add_argument(\"user-agent=fri-wier-oskapha\")\n",
    "        service = Service()\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        driver.set_page_load_timeout(10)\n",
    "        return driver\n",
    "        \n",
    "    def canonicalize_url(self, url):\n",
    "        \"\"\"\n",
    "        Canonicalizes a URL by removing redundant parts and ensuring consistency.\n",
    "        \"\"\"\n",
    "        parsed_url = urlparse(url)\n",
    "        parsed_url = parsed_url._replace(fragment='')\n",
    "        parsed_url = parsed_url._replace(scheme=parsed_url.scheme.lower(), netloc=parsed_url.netloc.lower())\n",
    "        parsed_url = parsed_url._replace(query='')\n",
    "        \n",
    "        if parsed_url.path.endswith('/'):\n",
    "            parsed_url = parsed_url._replace(path=parsed_url.path[:-1])\n",
    "        if parsed_url.path.endswith('/index.html'):\n",
    "            parsed_url = parsed_url._replace(path=parsed_url.path[:-11])\n",
    "        if parsed_url.path.endswith('/index.php'):\n",
    "            parsed_url = parsed_url._replace(path=parsed_url.path[:-10])\n",
    "        \n",
    "        if parsed_url.scheme == 'http' or parsed_url.scheme == 'https':\n",
    "            pass\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "        parsed_url = parsed_url._replace(path=urlparse(parsed_url.geturl()).path)\n",
    "        \n",
    "        canonicalized_url = urlunparse(parsed_url)\n",
    "        \n",
    "        return canonicalized_url\n",
    "    \n",
    "    def robots_allow(self, url):\n",
    "        domain = self.get_domain(url)\n",
    "        robot_parser = urllib.robotparser.RobotFileParser()\n",
    "        robot_parser.set_url(f'https://{domain}/robots.txt')\n",
    "        robot_parser.read()\n",
    "        return robot_parser.can_fetch(GROUP_NAME, url)\n",
    "    \n",
    "    def check_duplicates(self, page):\n",
    "        # Check if the page is already in the database by checking the content hash\n",
    "        content_hashes = []\n",
    "        # Get function to get the content hash of all pages in the database\n",
    "        \n",
    "        for content_hash in content_hashes:\n",
    "            if content_hash == page.content_hash:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "    \n",
    "    def get_domain(self, url):\n",
    "        parsed_url = urlparse(url)\n",
    "        domain = parsed_url.netloc\n",
    "        if \"www.\" in domain:\n",
    "            domain = domain.replace(\"www.\", \"\")\n",
    "            \n",
    "        return domain\n",
    "    \n",
    "    def get_frontier(self):\n",
    "        # Call database to get all the frontier entries from Page table\n",
    "        # For each push into frontier.Queue()\n",
    "        \n",
    "        print('Frontier size: %s', self.frontier.qsize())\n",
    "        \n",
    "    def set_frontier(self):\n",
    "        print('Setting frontier')\n",
    "\n",
    "        # Search the database\n",
    "        # Pages with PAGE_TYPE = FRONTIER\n",
    "        \n",
    "        # self.frontier.put(page)\n",
    "                \n",
    "        print('Frontier size: %s', self.frontier.qsize())\n",
    "        \n",
    "    def initialize_seed(self):\n",
    "        print('Initializing seed')\n",
    "        # Add URLs to frontier passed from the argument\n",
    "        # In case there are no URLs passed from the argument, search for the seed URLs in the database\n",
    "        if len(self.initial_seed) != 0:\n",
    "            for url in self.initial_seed:\n",
    "                #  Insert page into the frontier to DB\n",
    "                page = Page(url, self.get_domain(url))\n",
    "                \n",
    "                # Insert each page into frontier\n",
    "                \n",
    "                self.frontier.put(page)\n",
    "                \n",
    "        print('Frontier size: %s', self.frontier.qsize())\n",
    "        \n",
    "    def validate_links(self, links):\n",
    "        validated_links = []\n",
    "        for link in links:\n",
    "            #print('Orginial Link: ', link)\n",
    "            if link is not None and len(link) > 0 and 'javascript:' not in link.lower() and 'mailto:' not in link.lower():\n",
    "                domain = self.get_domain(link)\n",
    "                #print('Domain: ', domain)\n",
    "                if \"gov.si\" in domain: # Add robots check before appending\n",
    "                    # Removes /? \n",
    "                    #url = urldefrag(link)[0]\n",
    "                    \n",
    "                    canonicalized_url = self.canonicalize_url(link)\n",
    "                    #print('Canonicalized: ', canonicalized_url)\n",
    "                    \n",
    "                    # Check for duplicates in the array\n",
    "                    if (canonicalized_url not in validated_links or canonicalized_url not in self.initial_seed):\n",
    "                        #print('Valid')\n",
    "                        validated_links.append(canonicalized_url)\n",
    "            \n",
    "        return validated_links\n",
    "        \n",
    "    # Get all urls from links on the page\n",
    "    def get_links(self, page):\n",
    "        print('Getting links')\n",
    "        links = []\n",
    "        \n",
    "        # Check robots again\n",
    "        \n",
    "        # Get links from href\n",
    "        try:\n",
    "            hrefs = self.web_driver.find_elements(by=By.XPATH, value=\"//a[@href]\")\n",
    "            for href in hrefs:\n",
    "                try:\n",
    "                    href_value = href.get_attribute(\"href\")\n",
    "                    links.append(href_value)\n",
    "                except StaleElementReferenceException as e:\n",
    "                    pass\n",
    "        except NoSuchElementException:\n",
    "            print('No elements')    \n",
    "        \n",
    "        # Get links from buttons\n",
    "        # try:\n",
    "        #    buttons = self.web_driver.find_element(by=By.XPATH, value=\"//*[@onclick]\")\n",
    "        #    for button in buttons:\n",
    "        #        try:\n",
    "        #            click_value = button.get_attribute('onclick')\n",
    "        #            # print(\"Button: \", button)\n",
    "        #        except StaleElementReferenceException as e:\n",
    "        #            pass\n",
    "        #except NoSuchElementException:\n",
    "        #    print('No elements')\n",
    "        urls = []\n",
    "        soup = BeautifulSoup(page.html_content, 'html.parser')\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()\n",
    "        \n",
    "        for element in soup.select(\"[onClick], [onclick]\"):\n",
    "            print('Element: ', element)\n",
    "        \n",
    "        validated_links = self.validate_links(links)\n",
    "        \n",
    "        #for smt in validated_links:\n",
    "            #print(smt)\n",
    "            \n",
    "        return validated_links\n",
    "                \n",
    "    def crawl_webpage(self, page):\n",
    "        print('Started crawling', page.url)\n",
    "        \n",
    "        try:\n",
    "            # Implement robots.txt\n",
    "            \n",
    "            # Read the page\n",
    "\n",
    "            \n",
    "            response = requests.head(page.url, allow_redirects=True, timeout=10)\n",
    "            page.http_status_code = response.status_code\n",
    "            page.accessed_time = datetime.datetime.now()\n",
    "            page.content_type = response.headers['Content-Type']\n",
    "            \n",
    "            # Is it HTML\n",
    "            if \"text/html\" in response.headers['Content-Type']:\n",
    "                page.page_type_code = \"HTML\"\n",
    "                self.web_driver.get(page.url)\n",
    "                page.html_content = self.web_driver.page_source\n",
    "                \n",
    "                \n",
    "                # Check duplicates\n",
    "                \n",
    "                # Check links\n",
    "                links = self.get_links(page)\n",
    "\n",
    "                for link in links:\n",
    "                    # Insert page into frontier to DB\n",
    "                    page = Page(link, self.get_domain(link))\n",
    "                    #database.insert_page_into_frontier(conn, page.domain, page.url)\n",
    "\n",
    "                \n",
    "                \n",
    "                # Check for images\n",
    "                \n",
    "                # Insert links into frontier\n",
    "                \n",
    "                # Insert image data into database\n",
    "                \n",
    "            else:\n",
    "                # Else it is a binary file\n",
    "                print(\"It's a binary file!\")\n",
    "                \n",
    "                page.html_content = None\n",
    "                page.page_type_code = \"BINARY\"\n",
    "                if \"application/pdf\" == page.content_type:\n",
    "                    page.data_type = \"PDF\"\n",
    "                    \n",
    "                elif \"application/msword\" == page.content_type:\n",
    "                    page.data_type = \"DOC\"\n",
    "                \n",
    "                elif \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\" == page.content_type:\n",
    "                    page.data_type = \"DOCX\"\n",
    "                    \n",
    "                elif \"application/vnd.ms-powerpoint\" == page.content_type:\n",
    "                    page.data_type = \"PPT\"\n",
    "                    \n",
    "                elif \"application/vnd.openxmlformats-officedocument.presentationml.presentation\" == page.content_type:\n",
    "                    page.data_type = \"PPTX\"\n",
    "            \n",
    "                # Insert into database table Page Data since it is a binary file\n",
    "                \n",
    "                # Also update the page entry in table Page       \n",
    "        except Exception as e:\n",
    "            # Update database page entry with PAGE_TYPE = TIMEOUT\n",
    "            print('Error crawling %s: %s', page, e)\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bb23e398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start of crawler\n",
    "\n",
    "import urllib3\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "import warnings\n",
    "\n",
    "INITIAL_SEED = [\n",
    "    #'https://www.gov.si',\n",
    "    #'https://spot.gov.si',\n",
    "    #'https://e-uprava.gov.si',\n",
    "    'https://www.e-prostor.gov.si'\n",
    "]\n",
    "\n",
    "NUMBER_OF_WORKERS = 1\n",
    "\n",
    "GROUP_NAME='wier2024'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4bdee23a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing seed\n",
      "Frontier size: %s 1\n",
      "Started crawling https://www.e-prostor.gov.si\n",
      "Getting links\n",
      "Element:  <button aria-label=\"Išči\" id=\"search-button\" onclick=\"document.getElementById('form_kesearch_searchfield').submit();\" type=\"submit\"><span><i class=\"fal fa-search\"></i></span></button>\n",
      "Crawler finished\n"
     ]
    }
   ],
   "source": [
    "crawler = Crawler(initial_seed=INITIAL_SEED, num_workers=NUMBER_OF_WORKERS)\n",
    "crawler.initialize_seed()\n",
    "\n",
    "while crawler.frontier.qsize() > 0:\n",
    "    # Dequeue element\n",
    "    element = crawler.frontier.get()\n",
    "    \n",
    "    # Process the element\n",
    "    crawler.crawl_webpage(page=element)\n",
    "    \n",
    "    crawler.frontier.task_done()\n",
    "    \n",
    "    # Get frontier from database until frontier empty\n",
    "    \n",
    "print('Crawler finished')\n",
    "crawler.web_driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0892e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
